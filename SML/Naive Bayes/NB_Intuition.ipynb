{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is more centered around probability, using Bayes Theorem as the basis of inference of the probability of an event occuring given the occurence of other events. \n",
    "\n",
    "Naive Bayes does not consisder the correlation between events thus assumes that all events/features are independent of each other. \n",
    "\n",
    "This means that these probabilities can be multiplied together to get the joint propability, particularly with word vocabularies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Given a set of words w which tend to appear in 30 emails, you must determine whether an email is a spam email or not. \n",
    "\n",
    "Using Bayes theorem, we can determine that we will have a probability distribution of P(spam|X) = ( P(X|spam)* P(spam)) / P(X). The values are classified depending on the probability obtained. If the probability of the words involved being spam is greater than the probability of the words in the email not being spam, then the email is considered to a spam email i.e. P(spam|X) > P(not spam|X) => spam; P(not spam|X) > P(spam |X) => not spam. Thus the actual value is derived from the argmax of the conditional statement given an event X : \n",
    "\n",
    "Y = argmax(P(C|X)) == argmax(P(X|C)* P(C))\n",
    "\n",
    "Effectively, the probability of an event C occuring on condition X can thus be given by\n",
    "\n",
    "P(C|X) = product_of(P(C|X))\n",
    "\n",
    "If there are 10 spam emails and 20 not spam emsils,then P(spam) = 1/3; P(not spam) = 2/3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Using the MNIST datasets means that the values in the arrays will be between 0 and 255. For ease of computation, these values will need to be normalised to be between 0 and 1 which could be done by dividing by 255 or using the gaussian distribution equation: \n",
    "\n",
    "P(X) = (1/sqrt(2*pi*Variance(X))) * exp(-0.5((X-(np.mean(X)**2))/(variance(X))\n",
    "\n",
    "There is also the multivariate Gaussian distribution in which much like the standard gaussian distribution, works from determining the probability of the vector input X such that:\n",
    "\n",
    "P(X) = (1/(sqrt(pow(2*pi, D) * L1_norm(covariance))) * exp(-0.5 * ((X-np.mean(X)).T * 1/np.sum(X-np.mean(X)))\n",
    "\n",
    "The scipy library can calculate the multivariate gaussian distribution directly. \n",
    "\n",
    "NB: The covariance matrix shows the relationship between points in a vector and since Naive Bayes asssumes that these points are all independent, the values returned will be zero: Cov(i,j) = E[(x_i - np.mean(x_i))(x_j-np.mean(x_j)] = (- if x_i is independent of x_j)\n",
    "\n",
    "To combat this, we will need a D-size vector - an axis aligned elliptical covariance.\n",
    "\n",
    "Additionally since the probability is monotonically increasing, we can take the log of each and not suffer any consequences. \n",
    "\n",
    "\n",
    "Prediction = argmax_c{log(P(X|C) + log(P(C))}\n",
    "\n",
    "### Smoothing \n",
    "This tackles the singular covariance problem which arrises when you invert the matrix, adds numerical stability. To prevent this, multiply the identity matrix by a small number given by lambda e.g. 10**-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "def fit(X,Y):\n",
    "    dict_of_gaussians = {}\n",
    "    priors = {}\n",
    "    for c in classes:\n",
    "        Xc = X[corresponding Y == c]\n",
    "        mu, var = mean and diagnonal covariance of Xc\n",
    "        dict_of_gaussians[c] = {'mu':mu, 'var':var}\n",
    "        priors[c] = len(Xc)/len(X)  \n",
    "        \n",
    "def predict(X):\n",
    "    predictions = []\n",
    "    max_posterior = -inf\n",
    "    best_class = None\n",
    "    \n",
    "    for x in X: # loop through each value \n",
    "        for c in classes: # loop through all classes available\n",
    "            mu, var = dict_gaussians[c]\n",
    "            \n",
    "            #use mean and var to get the log probability distribution fnc\n",
    "            #added to log of priors to give posterior\n",
    "            posterior = log_pdf(x,mu,var) + log(priors[c])\n",
    "            if posterior > max_posterior:\n",
    "                max_posterior = posterior\n",
    "                best_class = c\n",
    "        predictions.append(best_class)\n",
    "    return predictions\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
