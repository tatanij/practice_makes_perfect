{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Deep Learning Intuition\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "Predicting possible class of that input data falls into between 2 class choices. If it is linearly separable (data of shape wTx) then Logistic Regression can be used to predict. \n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "Linear classification in more than 2-D is separable by a plane or hyper plabe. Neural networks are built to be able to deal with nonlinearly seperable data in which logistic regression methods are not appropriate. Neural networks can be built by combining multiple logistic regression units as the neurons to the network. Whilst polynomials like x^2 and x^3 are nonlinear, their properties are different from the deep neural network's neuron.\n",
    "\n",
    "\n",
    "The data given below is an example of a multiclass problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1500, 2)\n",
      "Shape of Y: (1500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "NClass = 500 # creating samples per class\n",
    "\n",
    "# creating gaussian clouds centered between the np.array()\n",
    "X1 = np.random.randn(NClass, 2) + np.array([0,-2])\n",
    "X2 = np.random.randn(NClass, 2) + np.array([2, 2])\n",
    "X3 = np.random.randn(NClass, 2) + np.array([-2, 2])\n",
    "\n",
    "X = np.vstack([X1,X2,X3])\n",
    "\n",
    "Y = np.array([0]*NClass +[1]*NClass + [2]*NClass)\n",
    "\n",
    "# visualise created data\n",
    "plt.scatter(X[:,0], X[:,1], c=Y, s=100, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dimensionalities for data\n",
    "D = 2\n",
    "M = 3 # hidden layer size\n",
    "K = 3 # number of classes\n",
    "\n",
    "# initialise weights\n",
    "W1 = np.random.randn(D,M) #\n",
    "W2 = np.random.randn(M,K)\n",
    "b1 = np.random.randn(M)\n",
    "b2 = np.random.randn(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above are randomly generated to containing 3 classes, of which there are 500 samples per class, resulting in 1500 samples. The sample data are 2D gaussain clouds centered at different x ranges. The weights are initalised randomly to adhere to dimensionality constraints. W1 is a 2 x 3 (DxM) matrix of randomly generated weights applied to X's 1500 x 2 matrix with the addition of the b1 bias to create 1500 x 3 matrix updated inputs which are then passed through activation function. \n",
    "\n",
    "W2 is a 3 x 3 (MxK) matrix which is applied to the output of the previous layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedfoward Network\n",
    "\n",
    "This is the most basic method of creating a neural network. The neuron in this method is considered to be like the logistic regression which follows equation $$a = x_1w_1 + x_2w_2 + b $$ and the prediction is made using the sigmoid funtion p(y|x) = 1(1+exp(-a)) as shown below\n",
    "\n",
    "<img src='../../images/sigmoid_fnc.jpg' width=\"20%\" height=\"20%\">\n",
    "\n",
    "A simple feedfoward network: \n",
    "\n",
    "<img src='../../images/feedforward.png' width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinearity functions\n",
    "\n",
    "The sigmoid function described above is known as an activation function which are also known to have nonlinear properties. The most popular of these functions include:\n",
    "- the sigmoid, which goes through 0 to 1; \n",
    "    <img src='../../images/sigmoid_graph.png' width=\"50%\" height=\"50%\">\n",
    "- the hyberpolic tangent (tanh) is another nonlinear function which instead goes through -1 to +1. Effectively the tanh is the sigmoid function stretched both horizontally and vertically\n",
    "    <img src='../../images/tanh_graph.png' width=\"50%\" height=\"50%\"> <img src='../../images/tanh_eq.png' width=\"20%\" height=\"20%\">\n",
    "- the relu, which returns 0 for any prediction less \n",
    "    <img src='../../images/relu_graph.jpg' width=\"30%\" height=\"30%\">\n",
    "#### Exercise : Show relationship between tanh and sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Logistic Regression for Multiclass Classification - Softmax\n",
    "\n",
    "In binary classification, the class can be represented as that with the highest probability of occurrence. This is given by the P(Y=1|X) and P(Y=0|X) = 1 - P(Y=1|X). This can also be done through two output nodes, exponentiate their values so that they are greater than 0 and then normalised to sum to 1. Using this method, the output of class 1 is given by:\n",
    "<img src='../../images/mult_node_c1.png' width=\"30%\" height=\"30%\">\n",
    "\n",
    "where a is the output of the activation function.\n",
    "\n",
    "Consequently, the output for class 2 can also be given by <img src='../../images/mult_node_c2n.png' width=\"30%\" height=\"30%\">\n",
    "which can all be summed to 1. The resultant weights will be matrix (Dx2) since every input node is connected to every output node with D input nodes and 2 output nodes (one for each class), the total number of weights is 2(D).\n",
    "\n",
    "For K classes, this can be further extended to host W = \\[w_1,w_2,...,w_k] (a DxK matrix) and much like with 2 output notes, you exponentiate every output and then normalising it as below:\n",
    "<img src='../../images/mult_node_k.png' width=\"30%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Feedforward Net\n",
    "\n",
    "The following example feed forward net uses the randomised data created above to create a single layer network with 3 hidden units and will return the probability of the occurence of the 3 classes. The feature data contains size of 1500, where each class contains 500 2D array samples, giving an 1500x2 input matrix.\n",
    "\n",
    "The first step in the forward pass involves the applying the randomly initialised weights and biases to the inputs and then applying the chosen activation function onto the modified inputs received by the node/neuron. This function gives rise to a 1500 x 3 matrix.\n",
    "\n",
    "Following this step, the output layer applies weights and biases to the output of the activation function and applies softmax to obtain a prediction. This function gives rise to a 1500 x 3 matrix.\n",
    "\n",
    "The classification of the data using the randomly generatered weights above will not result in a good classification rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate for randomly chosen weights: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../functions/')\n",
    "from activation_functions import sigmoid, softmax\n",
    "\n",
    "\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z = sigmoid((X.dot(W1) + b1))\n",
    "    # calculating softmax \n",
    "    A = Z.dot(W2) + b2\n",
    "    Yhat = softmax(A)\n",
    "    return Yhat,Z\n",
    "\n",
    "def classification(Y, Yhat):\n",
    "    n_correct = 0 # number of correctly classified values\n",
    "    n_total = 0 \n",
    "    for i in range(len(Y)):\n",
    "        n_total+=1\n",
    "        if Y[i] == Yhat[i]:\n",
    "            n_correct +=1 \n",
    "    return float(n_correct)/n_total\n",
    "\n",
    "P_Y_given_X = forward(X,W1,b1,W2,b2)[0]\n",
    "P = np.argmax(P_Y_given_X, axis=1)\n",
    "\n",
    "assert(len(P) == len(Y))\n",
    "\n",
    "print('Classification rate for randomly chosen weights:',classification(Y,P))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
